>   **SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: 1      |
|-----------------|
| Aleksander Rudolf                |   
| Anshdeep Singh              |   
| Jannine Osman               |   
| Jaskaran Bhatia                |   


**Table of Contents**

(When you finish writing, update the following list using right click, then
“Update Field”)

[1 Introduction	1](#_Toc439194677)

[2 High-level description of the exploratory testing plan	1](#_Toc439194678)

[3 Comparison of exploratory and manual functional testing	1](#_Toc439194679)

[4 Notes and discussion of the peer reviews of defect reports	1](#_Toc439194680)

[5 How the pair testing was managed and team work/effort was
divided	1](#_Toc439194681)

[6 Difficulties encountered, challenges overcome, and lessons
learned	1](#_Toc439194682)

[7 Comments/feedback on the lab and lab document itself	1](#_Toc439194683)

# Introduction

This laboratory experiment examines three distinct types of software testing techniques: exploratory testing, manual scripted testing, and regression testing. We employed two different versions of the same ATM system for our study. Prior to this lab, we had a general understanding of how exploratory testing works, as it was covered in our lecture. In exploratory testing, test cases are designed and executed simultaneously, without following a predefined script. The tester must familiarize themselves with the system's requirements before writing exploratory tests. Our understanding of manual scripted testing was that it involves a pre-established series of test cases, however, these tests are designed previously and executed later. One limitation of this method is that if a test case is overlooked, any bugs in that functionality may go undetected. Through this lab, we discovered how exploratory and manual scripted testing can complement each other to effectively identify bugs in the system. Additionally, we also covered regression testing, in which we re-ran all manual scripted tests to determine if any bugs were fixed in the newer version or if any new bugs were introduced.

# High-level description of the exploratory testing plan

Our team of 4 split up into two testing pairs and followed the same devised exploratory testing plan in order to gain knowledge and a deeper understanding of how the system functioned prior to undergoing manual scripted and regression testing. The high-level structure of this plan involved going through the processes of learning the system, designing the tests, and executing the tests simultaneously and was broken down into the following steps as follows:

1) Gain an understanding of the different functionalities and their requirements of the system and identify which all possible boundary cases of every functionality (Transaction) that may be prone to failure.
2) Identify the objectives of what testing those functionalities are. For our testing pairs it was to identify the triggers that caused failures and passes, based on our developing understanding of the overall system, and then design and execute further tests based on those findings.
3) Conduct testing the software in a dynamic and flexible approach in order to explore the different functionalities of the system to gain as much knowledge on the system as possible.
4) Document the findings and results of each test in order to be able to reproduce them at a later time and to use them to devise additional test cases.
5) Record all bugs/failures that the tests identify in Jira with the following format in order to create a comprehensive bug report that is to be used for comparing the bugs found in the exploratory testing phase to the bugs found in the manual scripted and regression testing phase. The format for bug reports is as follows;
The function being tested:
The initial state of the system:
Steps to reproduce the bug:
Expected outcome:
Actual outcome:
6) Communicate the exploratory testing results with the team to identify similarities and differences and merge the Jira bug reports.

# Comparison of exploratory and manual functional testing

The provided test suite had 40 tests that allowed us to test the system and determine the bugs and defects thoroughly. The test suite also provided a consistent way to test both versions (1.0 and 1.1) of the ATM machine so we could clearly determine what bugs were fixed. Our exploratory testing phase didn’t consider all functionalities/requirements the program should have. The manual testing phase was more efficient, consistent and effective when compared to our exploratory testing phase.

Our team had clear guidelines on each test case as well as how to reach that test case with the manual testing. Each test case was approached in a consistent manner as we knew what function we were testing, the required user input and the expected outcome. In our pairs, manually went through each test case, and recorded our results. We also did the manual testing in the order they were given so we could keep track of what tests came prior to the current test we were executing. The order of the test execution was crucial in ensuring we were not identifying incorrect bugs. For example, the deposit feature was not working correctly, so once the user deposited any amount of money, when we checked the balance of the account the balance was also wrong. If a customer was logged in initially and checked their balance before depositing any money the correct balance would be displayed. Since we knew the order we were executing the tests in we did not count this as a bug. This was not the case when testing in our exploratory phase. We did not clearly document everything we did before finding a bug, so the same situation above may have resulted in us incorrectly documenting a bug. We did not have as clear documentation in our early exploratory testing phase making it hard to formalize our bug reports in our Jira. When we first identified a bug in the exploratory testing phase, we often had to restart the system and resimulate the same test case multiple times to ensure it was a bug and not a result of human error.

The manual testing suite also allowed our team a consistent way to test both versions of the application. We manually went through all 40 test cases for each version and compared results to determine what was fixed in the new version. Identifying what was fixed from version 1.0 to 1.1 would be much more difficult if we didn’t have the test suite or if we tried to find all the changes through exploratory testing. This consistency was not guaranteed in our exploratory testing as we were simply just going through the requirements list manually, and not breaking down every situation into multiple cases. We also did not clearly document every step or test we did for our exploratory testing, so it would be very hard to accurately test version 1.1 of the system in the exact way we tested version 1.0.

Lastly, manual testing was efficient for our team as we had a clear plan of what needed to be tested and documented. During our exploratory testing, both teams were often finding the same defects that the other team had already found and documented. In the exploratory testing phase we often had to resimulate test cases as well to accurately document how we reached the bug. We also had some trouble initially documenting our bugs so our exploratory phase had us documenting bugs more than once, not documenting them at all, or documenting them in an unclear manner. This was not the case for manual testing. We clearly knew the steps to reach the bug, the input required and the expected output.

Furthermore, our team found manual testing was much more effective and consistent. We all had a clear plan and vision to follow to achieve testing all the requirements of the system. Since the test suite was clearly outlined and covered all the requirements of the system we were also able to test versions 1.0 and 1.1 in the same manner. Although we did not find the exploratory testing as efficient, the exploratory testing phase did allow our team to get familiar with how the system operates and some of its functionalities. The exploratory testing phase allowed each team member to familiarize themselves with the user interface, commands, credit card numbers and more. This made it much easier to complete the manual test phase as we each knew how to use the system prior to executing the tests in the test suite.

# Notes and discussion of the peer reviews of defect reports

Four members in the group are divided into two testing teams. Both of the teams went through all the practical functionalities of the ATM which are withdrawal, deposit, transfer and balance inquiry. As all four of these functionalities are important for real world applications such as ATM, the results need to be accurate. The main advantage of both the teams doing the same thing was that it was to locate errors. Looking at both reports, the functionalities which had the same value for a particular feature, we did not test that again. For example if we had to check for deposit in the checking account, and both of the testing teams got the same defect or bug, we marked that as a defect and mentioned it in our bug tracker (Jira). Now coming onto the case when the testing teams tested a particular functionality with different amounts. For example if we had to check for withdrawal from the saving account, and assuming testing team 1 checked for amounts $20, $40 and $60 and testing team 2 checked for amounts including $100 and $200. As both of the teams covered all the cases for withdrawal from saving accounts, we got to know some mathematical errors in the particular functionality as well which did not suit well for a real world ATM. So we mentioned that defect or bug in our backlog system (Jira). There were cases when one of the testing team caught a defect such as  receipt. For example in the withdrawal case or any deposit case, if we use a certain amount and we use card 1, the receipt always mentioned CARD2, and if we did the same transaction with card 2, the receipt showed CARD3. Some of the other general cases included wrong amount withdrawn, wrong amount deposited, wrong balance shown on the balance inquiry. There were minor bugs in the transaction receipt which were caught by one of the testing teams. This was majorly because of one testing team being faster than the other one and not putting all of their focus on minor details. For example, in the functionality called transfer funds, when we transfer from savings account to checking account, on the receipt it always shows the other way around. For the practical scenario it should mention transfer from saving to checking, in this ATM simulation it showed checkings to savings. In summary, having two different bug reports made it easier for the whole team to notice bugs as well as minor details which we would not have noticed if we were working as a single group.

# How the pair testing was managed and team work/effort was divided 

We divided ourselves into teams of two: 

Testing Team 1 - Anshdeep Singh and Jannine Osman
Testing Team 2 - Aleksander Rudolf and Jaskaran Singh Bhatia. 

Team 1 and Team 2 worked independently during the lab session. Both teams were unable to finish exploratory testing, so we created a discord server for the lab and each team joined a different channel to finish the exploratory testing. One member of each team conducted tests while the other member made notes of the results in the Backlog system (Jira).

When both teams finished testing, we discussed our results with each other. Most of the bugs were verified as they were reported by both teams. There were certain bugs that were found by a specific team. In that case, all four of us verified them and then added those bugs to Jira.

For manual scripted and regression testing, we again divided ourselves into pairs. The first pair worked on the first 20 MFTs, and the second pair worked on the last 20 MFTs. We followed a similar technique where one person tested and the other reported bugs to the bug tracking system. We also made sure that in regression testing, each pair would check the test cases they did while manual scripted testing, so they could make note of bugs that were fixed in the 1.1 version and update them in Jira.

After this, we all reviewed the issues and made sure everything was accurate. Later, we divided different portions of the report and worked on them individually, and each member reviewed the work of other team members.

# Difficulties encountered, challenges overcome, and lessons learned

One of the primary challenges we encountered was familiarizing ourselves with the system. Initially, our progress was slow, but as time passed, we grew accustomed to the system, making it easier to navigate. Another challenge we faced was testing the withdrawal function for card 1 and card 2, as there was a lack of consistency and we had to test various withdrawal amounts in order to identify a pattern. Furthermore, we began testing without fully reading the instructions, which resulted in the creation of unnecessary bugs. For instance, card 1 only had Checking and Savings accounts listed in the instructions, but we disregarded this information, ultimately leading to wasted time. In the beginning, we were uncertain about the necessity of using a backlog and whether the entries were related to the bugs. However, as time passed, these entries became clearer. A crucial lesson we learned was to thoroughly read the instructions and requirements before starting testing, which can save a significant amount of time. Additionally, we realized the importance of noting the version in which the bug was first identified and whether it was subsequently resolved in future versions.

# Comments/feedback on the lab and lab document itself

The lab is quite intriguing, as it offers us the chance to perform exploratory testing and understand how certain aspects can be overlooked during this process, as well as how certain functionality can be more effectively tested through manual scripting. The ATM system we used was user-friendly. The lab document, however, was quite verbose and difficult to understand initially. But, as we progressed through it step-by-step, it became more self-explanatory. Additionally, the use of the GitHub Classroom feature in our labs was beneficial as it allowed each group member to become more familiar with Git, which is becoming an increasingly dominant tool used by companies in the industry. It was also interesting to learn about Jira, as it is a useful software for tracking bugs and monitoring progress in rectifying them.
